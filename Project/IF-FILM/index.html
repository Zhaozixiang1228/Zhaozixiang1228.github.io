<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>IF-FILM (ICML 2024)</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Image Fusion via Vision-Language Model</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://zhaozixiang1228.github.io/" target="_blank">Zixiang Zhao</a><sup>1,2</sup>,</span>
              <span class="author-block">
                <a href="https://openreview.net/profile?id=~Lilun_Deng1" target="_blank">Lilun
                  Deng</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com.au/citations?user=gMSDelwAAAAJ&hl=en" target="_blank">Haowen
                  Bai</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://openreview.net/profile?id=~Yukun_Cui2" target="_blank">Yukun
                  Cui</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://openreview.net/profile?id=~Zhipeng_Zhang5" target="_blank">Zhipeng
                  Zhang</a><sup>2,3</sup>,</span>
              <span class="author-block">
                <a href="https://yulunzhang.com/" target="_blank">Yulun Zhang</a><sup>4</sup>,</span>
              <span class="author-block">
                <a href="https://htqin.github.io/" target="_blank">Haotong Qin</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="http://dongdongchen.com/" target="_blank">Dongdong Chen</a><sup>5</sup>,</span>
              <span class="author-block">
                <a href="http://gr.xjtu.edu.cn/web/jszhang" target="_blank">Jiangshe Zhang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com.au/citations?user=aPLp7pAAAAAJ&hl=en" target="_blank">Peng
                  Wang</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="https://vision.ee.ethz.ch/people-details.OTAyMzM=.TGlzdC8zMjQ4LC0xOTcxNDY1MTc4.html"
                  target="_blank">Luc Van Gool</a><sup>2,6,7</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <sup>1</sup>Xi'an Jiaotong University
                <sup>2</sup>ETH ZÃ¼rich
                <sup>3</sup>Northwestern Polytechnical University<br>
                <sup>4</sup>Shanghai Jiao Tong University
                <sup>5</sup>Heriot-Watt University
                <sup>6</sup>KU Leuven
                <sup>7</sup>INSAIT
                <br><b>ICML 2024</b></span>
              <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://openreview.net/pdf?id=eqY64Z1rsT" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/Zhaozixiang1228/IF-FILM" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2402.02235" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                <!-- Dataset link -->
                <span class="link-block">
                  <a href="https://github.com/Zhaozixiang1228/IF-FILM" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa fa-file-archive"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser image-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <center>
          <img src="static\images\1.png" border="0" width="70%">
        </center>
        <h2 class="subtitle has-text-centered">
          Figure 1: <b>Workflow for our <b>F</b>usion via v<b>I</b>sion-<b>L</b>anguage <b>M</b>odel
              <b>(FILM)</b>.</b> Input images are first processed to create prompts for ChatGPT, which
          then
          generate detailed textual descriptions. These descriptions help to get fused textual features via the frozen
          BLIP2 model. Then, these textual features are fused and guide the extraction and fusion of
          visual features via cross-attention, enhancing contextual understanding with text-based semantic information.
          Finally,
          the fusion image is output by the image decoder.
        </h2>
      </div>
    </div>
  </section>
  <!-- End teaser image -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Image fusion integrates essential information from multiple images into a single composite, enhancing
              structures,
              textures, and refining imperfections. Existing methods predominantly focus on pixel-level and semantic
              visual features
              for recognition, but often overlook the deeper text-level semantic information beyond vision. Therefore,
              we introduce a
              novel fusion paradigm named image <b>F</b>usion via v<b>I</b>sion-<b>L</b>anguage <b>M</b>odel
              <b>(FILM)</b>, for the first time, utilizing
              explicit
              textual information from source images to guide the fusion process. Specifically, FILM generates semantic
              prompts from
              images and inputs them into ChatGPT for comprehensive textual descriptions. These descriptions are fused
              within the
              textual domain and guide the visual information fusion, enhancing feature extraction and contextual
              understanding,
              directed by textual semantic information via cross-attention. FILM has shown promising results in four
              image fusion
              tasks: infrared-visible, medical, multi-exposure, and multi-focus image fusion. We also propose a
              vision-language
              dataset containing ChatGPT-generated paragraph descriptions for the eight image fusion datasets across
              four fusion
              tasks, facilitating future research in vision-language model-based image fusion.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->


  <!-- Architecture of FILM -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Architecture of FILM</h2>
        <center>
          <img src="static\images\2.png" border="0" width="80%">
        </center>
        <h2 class="subtitle has-text-centered">
          Figure 2: <b>Network pipeline for our FILM</b>, which encompasses three components: text paragraph generation
          and
          text feature fusion,
          language-guided vision feature fusion via cross attention and vision feature decoding, corresponding to the
          first,
          second, and third columns.
        </h2>
      </div>
    </div>
  </section>
  <!-- End Architecture of FILM -->


  <!-- Vision-Language Fusion Dataset -->
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Vision-Language Fusion (VLF) Dataset</h2>
        <h2 class="content has-text-justified">
          <p>
            Considering the high computational cost of invoking various vision-language components, and to facilitate
            subsequent
            research on image fusion based on vision-language models, we propose the <b>VLF Dataset</b>. This dataset
            encompasses paired
            paragraph descriptions generated by ChatGPT, covering all image pairs from the training and test sets of the
            eight
            widely-used fusion datasets.</p>
          <p>
            These include paragraph descriptions of:
          <ul>
            <li><b>Infrared-visible image fusion (IVF)</b>:
              <a href="https://github.com/Linfeng-Tang/MSRS" target="_blank" style="color: hsl(204, 86%, 53%)">MSRS</a>,
              <a href="https://github.com/JinyuanLiu-CV/TarDAL" target="_blank"
                style="color: hsl(204, 86%, 53%)">M<sup>3</sup>FD</a>,
              and
              <a href="https://github.com/hanna-xu/RoadScene" target="_blank"
                style="color: hsl(204, 86%, 53%)">RoadScene</a> datasets;
            </li>
            <li><b>Medical image fusion (MIF)</b>:
              <a href="http://www.med.harvard.edu/AANLIB/home.html" target="_blank"
                style="color: hsl(204, 86%, 53%)">Harvard </a> dataset;
            </li>
            <li><b>Multi-exposure image fusion (MEF)</b>:
              <a href="https://github.com/csjcai/SICE" target="_blank" style="color: hsl(204, 86%, 53%)">SICE</a> and
              <a href="https://github.com/xingchenzhang/MEFB" target="_blank" style="color: hsl(204, 86%, 53%)">MEFB</a>
              datasets.
            </li>
            <li><b>Multi-focus image fusion (MFF)</b>:
              <a href="https://github.com/Zancelot/Real-MFF" target="_blank"
                style="color: hsl(204, 86%, 53%)">RealMFF</a> and
              <a href="http://mansournejati.ece.iut.ac.ir/content/lytro-multi-focus-dataset" target="_blank"
                style="color: hsl(204, 86%, 53%)">Lytro</a> datasets;
            </li>

          </ul>
          </p>
          <p>
            <b>The dataset is available for download via
              <a href="" target="_blank" style="color: hsl(204, 86%, 53%)">Google Drive</a>.</b>
          </p>
          <p>
          <u><b>[Notice]</b></u>:
          <b>Considering the immense workload involved in creating this dataset, we have opened a 
            <a href="https://forms.gle/1kTAS15DktRPLs5BA" target="_blank" style="color: hsl(204, 86%, 53%)">Google Form</a> for error correction feedback. Please
          provide your suggestions for correcting any errors in the VLF dataset. If you have any questions regarding the Google
          Form, please contact <a href="https://zhaozixiang1228.github.io/" target="_blank" style="color: hsl(204, 86%, 53%)">Zixiang Zhao</a> via email.</b>
          </p>
          <p>Visualization of the VLF dataset:</p>
        </h2>
        <center>
          <img src="static\images\3.png" border="0" width="100%">
        </center>
        <h2 class="subtitle has-text-centered">
          Figure 3: Visualization of the VLF dataset creation process and representative data displays.
        </h2>
        <h2 class="content has-text-justified">
          More detailed images of the VLF dataset:
        </h2>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <!-- Your image here -->
            <center>
              <img src="static\images\SM1.png" alt="MY ALT TEXT" width="60%" />
            </center>
            <h2 class="subtitle has-text-centered">
              More visualization results for the VLF dataset on IVF and MFF.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <center>
              <img src="static\images\SM2.png" alt="MY ALT TEXT" width="60%" />
            </center>
            <h2 class="subtitle has-text-centered">
              More visualization results for the VLF dataset on MEF and MIF.
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Vision-Language Fusion Dataset -->




  <!-- Experimental results -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Experimental Results</h2>

        <!-- IVF -->
        <h2 class="content has-text-justified">
          <p>
            <u><b>Infrared-visible image fusion (IVF):</b></u>
          </p>
        </h2>
        <center>
          <img src="static\images\R1.png" border="0" width="100%">
        </center>

        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <!-- Your image here -->
            <center>
              <img src="static\images\RS_06570.png" alt="MY ALT TEXT" width="100%" />
            </center>
            <h2 class="subtitle has-text-centered">
              Visualization comparison of the fusion results in the infrared-visible image fusion task.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <center>
              <img src="static\images\RS_04354.png" alt="MY ALT TEXT" width="100%" />
            </center>
            <h2 class="subtitle has-text-centered">
              Visualization comparison of the fusion results in the infrared-visible image fusion task.
            </h2>
          </div>
        </div>

        <!-- MIF -->
        <h2 class="content has-text-justified">
          <p>
            <u><b>Medical image fusion (MIF):</b></u>
          </p>
        </h2>
        <center>
          <img src="static\images\R2.png" border="0" width="50%">
        </center>

        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <!-- Your image here -->
            <center>
              <img src="static\images\Harvard_MRI_PET_19.png" alt="MY ALT TEXT" width="50%" />
            </center>
            <h2 class="subtitle has-text-centered">
              Visualization comparison of the fusion results in the medical image fusion task.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <center>
              <img src="static\images\Harvard_MRI_PET_44.png" alt="MY ALT TEXT" width="100%" />
            </center>
            <h2 class="subtitle has-text-centered">
              Visualization comparison of the fusion results in the medical image fusion task.
            </h2>
          </div>
        </div>

        <!-- MEF -->
        <h2 class="content has-text-justified">
          <p>
            <u><b>Multi-exposure image fusion (MEF):</b></u>
          </p>
        </h2>
        <center>
          <img src="static\images\R3.png" border="0" width="100%">
        </center>

        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <!-- Your image here -->
            <center>
              <img src="static\images\SICE_076.png" alt="MY ALT TEXT" width="100%" />
            </center>
            <h2 class="subtitle has-text-centered">
              Visualization comparison of the fusion results in the multi-exposure image fusion task.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <center>
              <img src="static\images\MEFB_Candle.png" alt="MY ALT TEXT" width="100%" />
            </center>
            <h2 class="subtitle has-text-centered">
              Visualization comparison of the fusion results in the multi-exposure image fusion task.
            </h2>
          </div>
        </div>

        <!-- MFF -->
        <h2 class="content has-text-justified">
          <p>
            <u><b>Multi-focus image fusion (MFF):</b></u>
          </p>
        </h2>
        <center>
          <img src="static\images\R4.png" border="0" width="100%">
        </center>

        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <!-- Your image here -->
            <center>
              <img src="static\images\RealMFF_126.png" alt="MY ALT TEXT" width="100%" />
            </center>
            <h2 class="subtitle has-text-centered">
              Visualization comparison of the fusion results and error maps in multi-focus image fusion task.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <center>
              <img src="static\images\RealMFF_024.png" alt="MY ALT TEXT" width="100%" />
            </center>
            <h2 class="subtitle has-text-centered">
              Visualization comparison of the fusion results and error maps in multi-focus image fusion task.
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Experimental results -->


  <!--BibTex citation -->
  <section class="hero is-small" id="BibTeX">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">BibTeX</h2>
        <pre><code>
      @inproceedings{Zhao_2024_ICML,
        title={Image Fusion via Vision-Language Model},
        author={Zixiang Zhao and Lilun Deng and Haowen Bai and Yukun Cui and Zhipeng Zhang and Yulun Zhang and Haotong Qin and Dongdong Chen and Jiangshe Zhang and Peng Wang and Luc Van Gool},
        booktitle={Proceedings of the International Conference on Machine Learning (ICML)},
        year={2024},
      }
        </code></pre>
      </div>
    </div>
  </section>
  <!--End BibTex citation -->

  <section class="hero is-small" id="Citaton">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Related Works</h2>
        <h2 class="content has-text-justified">
          <ul>
            <li><a
                href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhao_Equivariant_Multi-Modality_Image_Fusion_CVPR_2024_paper.html">
                Equivariant Multi-Modality Image Fusion</a>.
              <b>CVPR 2024.</b><br>
              Zixiang Zhao, Haowen Bai, Jiangshe Zhang, Yulun Zhang, Kai Zhang, Shuang Xu, Dongdong Chen, Radu Timofte,
              Luc Van Gool
              <div class="bibtex-box">
  @inproceedings{Zhao_2024_CVPR,
  &nbsp;&nbsp;author = {Zhao, Zixiang and Bai, Haowen and Zhang, Jiangshe and Zhang, Yulun and Zhang, Kai and Xu, Shuang and Chen, Dongdong and Timofte, Radu and Van Gool, Luc},
  &nbsp;&nbsp;title = {Equivariant Multi-Modality Image Fusion},
  &nbsp;&nbsp;booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  &nbsp;&nbsp;month = {June},
  &nbsp;&nbsp;year = {2024},
  &nbsp;&nbsp;pages = {25912-25921}
  }
              </div>
            </li>

            <li><a
                href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_DDFM_Denoising_Diffusion_Model_for_Multi-Modality_Image_Fusion_ICCV_2023_paper.html">
                DDFM: Denoising Diffusion Model for Multi-Modality Image
                Fusion</a>.
              <b>ICCV 2023 (Oral).</b><br>
              Zixiang Zhao, Haowen Bai, Yuanzhi Zhu, Jiangshe Zhang, Shuang Xu, Yulun Zhang, Kai Zhang, Deyu Meng, Radu
              Timofte, Luc Van Gool.
              <div class="bibtex-box">
  @inproceedings{Zhao_2023_ICCV,
  &nbsp;&nbsp;author = {Zhao, Zixiang and Bai, Haowen and Zhu, Yuanzhi and Zhang, Jiangshe and Xu, Shuang and Zhang, Yulun and Zhang, Kai and Meng, Deyu and Timofte, Radu and Van Gool, Luc},
  &nbsp;&nbsp;title = {DDFM: Denoising Diffusion Model for Multi-Modality Image Fusion},
  &nbsp;&nbsp;booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  &nbsp;&nbsp;month = {October},
  &nbsp;&nbsp;year = {2023},
  &nbsp;&nbsp;pages = {8082-8093}
  }
              </div>
            </li>

            <li><a
                href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhao_CDDFuse_Correlation-Driven_Dual-Branch_Feature_Decomposition_for_Multi-Modality_Image_Fusion_CVPR_2023_paper.html">
                CDDFuse: Correlation-Driven Dual-Branch Feature
                Decomposition for Multi-Modality Image Fusion</a>.
              <b>CVPR 2023.</b><br>
              Zixiang Zhao, Haowen Bai, Jiangshe Zhang, Yulun Zhang, Shuang Xu, Zudi Lin, Radu Timofte, Luc Van Gool.
              <div class="bibtex-box">
  @inproceedings{Zhao_2023_CVPR,
  &nbsp;&nbsp;author = {Zhao, Zixiang and Bai, Haowen and Zhang, Jiangshe and Zhang, Yulun and Xu, Shuang and Lin, Zudi and Timofte, Radu and Van Gool, Luc},
  &nbsp;&nbsp;title = {CDDFuse: Correlation-Driven Dual-Branch Feature Decomposition for Multi-Modality Image Fusion},
  &nbsp;&nbsp;booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  &nbsp;&nbsp;month = {June},
  &nbsp;&nbsp;year = {2023},
  &nbsp;&nbsp;pages = {5906-5916}
  }
              </div>
              
            </li>
            <li><a href="https://www.ijcai.org/Proceedings/2020/135"> DIDFuse: Deep Image Decomposition for Infrared and
                Visible Image Fusion</a>.
              <b>IJCAI 2020.</b><br>
              Zixiang Zhao, Shuang Xu, Chunxia Zhang, Junmin Liu, Jiangshe Zhang and Pengfei Li.
              <div class="bibtex-box">
  @inproceedings{DBLP:conf/ijcai/ZhaoXZLZL20,
  &nbsp;&nbsp;author = {Zixiang Zhao and Shuang Xu and Chunxia Zhang and Junmin Liu and Jiangshe Zhang and Pengfei Li},
  &nbsp;&nbsp;title = {DIDFuse: Deep Image Decomposition for Infrared and Visible Image Fusion},
  &nbsp;&nbsp;booktitle = {Proceedings of the International Joint Conference on Artificial Intelligence ({IJCAI})},
  &nbsp;&nbsp;pages = {970--976},
  &nbsp;&nbsp;year = {2020}
  }
              </div>
            </li>
            <li><a href="https://ieeexplore.ieee.org/document/9416456"> Efficient and Model-Based Infrared and Visible
                Image Fusion via Algorithm Unrolling</a>.
              <b>IEEE Transactions
                on Circuits
                and Systems for Video Technology 2021.</b><br>
              Zixiang Zhao, Shuang Xu, Jiangshe Zhang, Chengyang Liang, Chunxia Zhang and Junmin Liu.
              <div class="bibtex-box">
  @article{zhao2021efficient,
  &nbsp;&nbsp;title = {Efficient and model-based infrared and visible image fusion via algorithm unrolling},
  &nbsp;&nbsp;author = {Zhao, Zixiang and Xu, Shuang and Zhang, Jiangshe and Liang, Chengyang and Zhang, Chunxia and Liu, Junmin},
  &nbsp;&nbsp;journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  &nbsp;&nbsp;volume = {32},
  &nbsp;&nbsp;number = {3},
  &nbsp;&nbsp;pages = {1186--1196},
  &nbsp;&nbsp;year = {2021},
  &nbsp;&nbsp;publisher = {IEEE}
  }
              </div>
            </li>
            <!-- <li><a href="https://robustart.github.io/long_paper/26.pdf"> Deep Convolutional Sparse Coding Networks for Interpretable Image Fusion</a>.
              <b>CVPR Workshop 2023.</b>
              Zixiang Zhao, Jiangshe Zhang, Haowen Bai, Yicheng Wang, Yukun Cui, Lilun Deng, Kai Sun, Chunxia Zhang,
              Junmin Liu,
              Shuang Xu.
            </li>
            <li><a href="https://doi.org/10.1016/j.sigpro.2020.107734"> Bayesian fusion for infrared and visible images</a>.
              <b>Signal Processing.</b>
              Zixiang Zhao, Shuang Xu, Chunxia Zhang, Junmin Liu, Jiangshe Zhang.
            </li> -->
          </ul>
        </h2>
      </div>
    </div>
  </section>

  <script>
    document.querySelectorAll('li').forEach(item => {
      item.addEventListener('mouseenter', event => {
        const bibtexBox = item.querySelector('.bibtex-box');
        bibtexBox.style.display = 'block';
      });

      item.addEventListener('mouseleave', event => {
        const bibtexBox = item.querySelector('.bibtex-box');
        bibtexBox.style.display = 'none';
      });
    });
  </script>

  <section class="hero is-small is-light" id="License">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">License</h2>
        <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank"><img
            alt="Creative Commons License" style="border-width:0"
            src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a></br>
        FILM is licensed under a <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" style="color: hsl(204, 86%, 53%)">CC BY-NC-SA
          4.0 License</a>.
      </div>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              <center>
                <font size=4>Â© Zixiang Zhao | Last updated: 27/6/2024</font>
              </center>
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>